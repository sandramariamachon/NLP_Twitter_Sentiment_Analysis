{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweepy Twitter Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks contains the Tweepy scraper for Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F98DB7tZLuur"
   },
   "outputs": [],
   "source": [
    "import tweepy # for tweet mining\n",
    "import csv # to read and write csv files\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uSDuYXeMc3i"
   },
   "source": [
    "## **Authorisation Codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTq81RCqMVAa"
   },
   "outputs": [],
   "source": [
    "# Consumer_key and access_key generated from Twitter's developer account\n",
    "CONSUMER_KEY = 'XXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "CONSUMER_SECRET = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "ACCESS_KEY = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "ACCESS_SECRET = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VUREsNpMVDa"
   },
   "outputs": [],
   "source": [
    "# Authorisation code\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) # Pass in Consumer key and secret for authentication by API\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET) # Pass in Access key and secret for authentication by API\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True) # Sleeps when API limit is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeBxZQEBMo4P"
   },
   "source": [
    "## **Scraping Tweets from API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6mO5SRWMVIr"
   },
   "outputs": [],
   "source": [
    "def get_tweets_in_data_range(data, phrase, max_tweets, end_date, start_date = None, start_via_tweet_id = None):\n",
    "  \"\"\"Scrapes Tweets based on given phrase, beteen dates. Alternatively this can work via a range of start_tweet_id and end date\"\"\"\n",
    "  search_query = phrase + \" -filter:links AND -filter:retweets AND -filter:replies\" #Exclude links, retweets, replies\n",
    "  for i in tweepy.Cursor(api.search, q = search_query, since = start_date, until = end_date, since_id = start_via_tweet_id, lang = \"en\", tweet_mode = \"extended\").items(max_tweets):\n",
    "    data.append([i.full_text, i.id, i.created_at, i.coordinates, i.retweet_count, i.favorite_count]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIX0WIOCMVLa"
   },
   "outputs": [],
   "source": [
    "scraped_data = []\n",
    "scraped_data.append([\"text\", \"id\", \"time\", \"location\", \"retweet_count\", \"fav_count\"])\n",
    "PHRASE = \"\\\"Covid-19\\\"\"\n",
    "MAX_TWEETS = 1000 #Maximum number of tweets to scrape\n",
    "START_DATE = '2021-03-25' #only last 7 days supported\n",
    "END_DATE = '2021-03-26' #only last 7 days supported\n",
    "get_tweets_in_data_range(scraped_data, PHRASE, MAX_TWEETS, END_DATE, START_DATE) #call to get tweets between date ranges \n",
    "#get_tweets_in_data_range(scraped_data, PHRASE, MAX_TWEETS, END_DATE, None, 1375235706382123011) #call to get tweets from a tweet ID to end date (note that the Start_date is set to \"None\" and an a start_via_tweet_id argument is given)\n",
    "#get_tweets_in_data_range(scraped_data, PHRASE, MAX_TWEETS, END_DATE, None, scraped_data[1][1]) #use scraped_data[1][1] after calling \"get_tweets_in_data_range\" once to access the latest id from the last call (to update with latest data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "essTrkl_TiLa"
   },
   "source": [
    "## **Save to CSV via Pandas data frame**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lnvHBG1QUZL"
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(scraped_data[1:],columns=scraped_data[0])\n",
    "tweets_csv = tweets.to_csv('scraped_data.csv', index=True)  #saves a csv file with the data scraped\n",
    "#print(tweets)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMqD9uUVDMZqsfvQhTxxD4t",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Tweepy_Twitter_Scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
